---
title: "Predicting CBB Winners"
author: "Jake Whalen"
date: "May 17, 2018"
output: 
  html_document: 
    df_print: kable
    highlight: zenburn
    theme: yeti
    toc: yes 
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(magrittr)
library(readr)
library(parallel)
```

# Motivation

I love college basketball.
Specifically UVA basketball.
Go HOOS!

# Data

Thank you to the Kaggle March Madness competitions.
Pulled all the data just prior to the tournament in 2018.

## Parallel Read

First I gathered the list of file names stored in the data folder in this repository.

```{r}
dataFolder <- paste(getwd(), '/data', sep='')
dataFiles <- list.files(dataFolder)
```

I then initiate a cluster using the CPU nodes on my machine.
I subtract 1 from the total number of CPU nodes to avoid accidentally maxing out my processing power when other applications are running.

```{r}
cl <- makeCluster(detectCores()-1)
```

When running a process in parallel you must ensure each node running the process has all the resources it needs.
The code below loads the package for reading the data as well as the list of file names to read in.
I honestly do not know why the NULL is passed to each node.
I think this is printed out and ensures the right number of nodes are running in the cluster.

```{r, results='hide'}
clusterEvalQ(cl, {
    library(readr)
    dataFolder <- paste(getwd(), '/data', sep='')
    dataFiles <- list.files(dataFolder)
    NULL
})
```

Next I must build a function that will then run on each node.
The function in this case is a simple read in of a csv file from a list of file names.

```{r}
readFiles <- function(fileName) {
    read.csv(file=paste(dataFolder, fileName, sep="/"))
}
```

Using the function `parSapply` I can pass the inputs and function into the cluster and the R code for the `parSapply` function takes care of scheduling the nodes and what files they will read in.
The results (data frames) will be stores in the list `cbb`.

```{r}
cbb <- parSapply(cl, dataFiles, readFiles)
```

After the job completes I make sure to shut the cluster down in order to avoid future issues with the CPU.

```{r}
stopCluster(cl)
```


# Path Forward


